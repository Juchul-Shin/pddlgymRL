{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from random import randint\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import problem_generator\n",
    "import pddlgym\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 방향과 액션 predicate 상수 리스트(numeric <-> string)\n",
    "DIR_LIST = ['north', 'east', 'south', 'west']\n",
    "ACT_LIST = ['heading-forward', 'heading-left', 'heading-right']\n",
    "\n",
    "# Invalid Direction에 대한 예외 처리용\n",
    "class InvalidDirection(Exception):\n",
    "    \"\"\"See PDDLEnv docstring\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# north를 기준으로 x,y 좌표 값을 회전 시킴\n",
    "def rotate_coordinate(coordinate, direction, grid_size = 5):\n",
    "    if (direction=='east'):\n",
    "        t_x = grid_size - coordinate[1] + 1\n",
    "        t_y = coordinate[0]\n",
    "        return (t_x, t_y)\n",
    "    elif (direction=='south'):\n",
    "        t_x = grid_size - coordinate[0] + 1\n",
    "        t_y = grid_size - coordinate[1] + 1\n",
    "        return (t_x, t_y)\n",
    "    elif (direction=='west'):\n",
    "        t_x = coordinate[1]\n",
    "        t_y = grid_size - coordinate[0] + 1\n",
    "        return (t_x, t_y)\n",
    "    elif (direction=='north'):\n",
    "        return coordinate\n",
    "    else:\n",
    "        raise InvalidDirection(f\"direction '{direction}' is invalid\")\n",
    "\n",
    "# 상대 좌표 계산\n",
    "def relative_coordinate(origin, object):\n",
    "        return (object[0]-origin[0], object[1]-origin[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularRL:\n",
    "    def __init__(self, env, default_action_value = -10., grid_size=5, num_threat = 1, num_target = 0):\n",
    "        self.env = env\n",
    "        self.action_value = defaultdict(lambda : [default_action_value, default_action_value, default_action_value])\n",
    "        self.action_space = env.action_space.predicates\n",
    "        self.grid_size = grid_size\n",
    "        self.num_threat = num_threat\n",
    "        self.num_target = num_target\n",
    "        #self.target_acquired = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # PDDL Observation을 Grid의 Orient, Origin을 North, Drone 위치를 중심으로 좌표 변환하여 Tuple로 만들어 반환\n",
    "    def _convert_obs(self, obs):\n",
    "        for element in obs[0]:\n",
    "            if (element.predicate.name == 'drone-at'):\n",
    "                drone_at = element\n",
    "            if (element.predicate.name == 'drone-to'):\n",
    "                drone_to = element\n",
    "            if (element.predicate.name == 'threat-at'):\n",
    "                threat_at = element\n",
    "            if (element.predicate.name == 'target-at'):\n",
    "                target_at = element\n",
    "        drone_x = int(drone_at._str.split(':')[0].split('-')[2])\n",
    "        drone_y = int(drone_at._str.split(':')[0].split('-')[3])\n",
    "        drone_to = drone_to._str.split('(')[1].split(':')[0]\n",
    "        #get goal,threat position\n",
    "        goal_x = int(obs[2]._str.split(':')[0].split('-')[2])\n",
    "        goal_y = int(obs[2]._str.split(':')[0].split('-')[3])\n",
    "        threat_x = int(threat_at._str.split(':')[0].split('-')[2])\n",
    "        threat_y = int(threat_at._str.split(':')[0].split('-')[3])\n",
    "        target_x = int(target_at._str.split(':')[0].split('-')[2])\n",
    "        target_y = int(target_at._str.split(':')[0].split('-')[3])\n",
    "        #transform coordinate by direction        \n",
    "        transformed_drone = rotate_coordinate((drone_x, drone_y), drone_to, self.grid_size)\n",
    "        transformed_goal = rotate_coordinate((goal_x, goal_y), drone_to, self.grid_size)\n",
    "        transformed_threat = rotate_coordinate((threat_x, threat_y), drone_to, self.grid_size)\n",
    "        transformed_target = rotate_coordinate((target_x, target_y), drone_to, self.grid_size)\n",
    "        #get relative coordinate from drone to goal\n",
    "\n",
    "        \n",
    "        converted_obs = (*transformed_drone, DIR_LIST.index(drone_to), *transformed_goal)\n",
    "\n",
    "        if (self.num_threat > 0):\n",
    "            converted_obs = (*converted_obs, *transformed_threat)\n",
    "        if (self.num_target > 0):\n",
    "            if (not self.target_acquired):\n",
    "                if (transformed_target[0] == transformed_drone[0]) and (transformed_target[1] == transformed_drone[1]):\n",
    "                    self.target_acquired = True\n",
    "            converted_obs = (*converted_obs, *transformed_target, self.target_acquired)\n",
    "        \n",
    "        return converted_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #### valid actions이 아닌 index가 뽑힌 경우 no greedy action에 빠짐\n",
    "    def greedy(self, state, valid_actions):\n",
    "        q_values = []\n",
    "        if (not (state in self.action_value)): # 처음 방문하는 state는 invalid action 처리 후 랜덤 액션 반환\n",
    "            # state에서 valid action이 아닌 경우는 큰 음의 값을 설정\n",
    "            q_values = np.array(self.action_value[state])\n",
    "            valid_action_list = [action.predicate.name for action in valid_actions]\n",
    "            for index, action in enumerate(ACT_LIST):\n",
    "                if not action in valid_action_list:\n",
    "                    q_values[index] = np.NINF\n",
    "                    self.action_value[state][index] = np.NINF\n",
    "            action = random.choice(valid_actions)\n",
    "            index = ACT_LIST(action.predicate.name)\n",
    "            return action, index\n",
    "        \n",
    "        # 기존 방문 state에서는 #Q값이 가장 큰 valid action 선택\n",
    "        q_values = np.array(self.action_value[state])\n",
    "        sorted_indices = np.argsort(q_values)\n",
    "\n",
    "        for i in range(0,3):\n",
    "            index = sorted_indices(-i)\n",
    "            for action in valid_actions:\n",
    "                if (action.predicate.name == ACT_LIST[index]):\n",
    "                    return action, index\n",
    "\n",
    "        print(\"****** There is no greedy action *****\")\n",
    "        print(self.env)\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def e_greedy(self, state, epsilon, valid_actions):\n",
    "        '''\n",
    "        Epsilon greedy policy\n",
    "        '''\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            # Choose a random action\n",
    "            action = random.choice(valid_actions)\n",
    "            for index, action_name in enumerate(ACT_LIST):\n",
    "                if action_name == action.predicate.name:\n",
    "                    return action, index\n",
    "        else:\n",
    "            # Choose the action of a greedy policy\n",
    "            return self.greedy(state, valid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 현재 Q-Value에 대한 Greedy Policy로 에피소드를 num_episodes 마늠 수행\n",
    "    def run_episodes(self, env, num_episodes, to_print = False):\n",
    "        total_rewards = []\n",
    "        for e in range(num_episodes):\n",
    "            obs, _ = env.reset()\n",
    "            tabular_state = self._convert_obs(obs)\n",
    "            done = False\n",
    "            game_reward = 0\n",
    "            num_steps = 0      #Step 수를 제한\n",
    "            self.target_acquired = False\n",
    "\n",
    "            while not done:\n",
    "                # select a greedy action\n",
    "                valid_actions = list(sorted(env.action_space.all_ground_literals(obs, valid_only=True)))\n",
    "                action, _ = self.greedy(tabular_state, valid_actions)\n",
    "                if to_print:\n",
    "                    self.printobs(obs)\n",
    "                    print(action)\n",
    "                next_obs, rew, done, _ = env.step(action)\n",
    "                obs = next_obs\n",
    "                tabular_state = self._convert_obs(obs)\n",
    "\n",
    "                if self.num_target > 0:\n",
    "                    if ((not done) and (not self.target_acquired)):\n",
    "                        reward = 10\n",
    "                    elif (done and self.target_acquired):\n",
    "                        reward = 20\n",
    "\n",
    "                game_reward += rew \n",
    "                \n",
    "                if done or num_steps > 50:\n",
    "                    total_rewards.append(game_reward)\n",
    "                    if to_print and done and self.target_acquired:\n",
    "                        print(\"Agent has arrived at the goal position acquiring the target successfully\\n\\n\")\n",
    "                    elif to_print and done:\n",
    "                        print(\"Agent has arrived at the goal position\\n\\n\")\n",
    "                    elif to_print:\n",
    "                        print(\"Agent couldn't arrive at \")\n",
    "                    break\n",
    "                num_steps += 1\n",
    "\n",
    "        if to_print:\n",
    "            print('Mean score: %.3f of %i games!'%(np.mean(total_rewards), num_episodes))\n",
    "\n",
    "        return np.mean(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 현재 observation 중 grid와 관련된 내용 출력\n",
    "    def printobs(self, obs):\n",
    "        for element in obs[0]:\n",
    "            if (element.predicate.name == 'drone-at'):\n",
    "                drone_at = element\n",
    "            if (element.predicate.name == 'drone-to'):\n",
    "                drone_to = element\n",
    "            if (element.predicate.name == 'threat-at'):\n",
    "                threat_at = element\n",
    "            if (element.predicate.name == 'target-at'):\n",
    "                target_at = element\n",
    "\n",
    "        drone_x = int(drone_at._str.split(':')[0].split('-')[2])\n",
    "        drone_y = int(drone_at._str.split(':')[0].split('-')[3])\n",
    "        drone_to = drone_to._str.split('(')[1].split(':')[0]\n",
    "        #get goal position\n",
    "        goal_x = int(obs[2]._str.split(':')[0].split('-')[2])\n",
    "        goal_y = int(obs[2]._str.split(':')[0].split('-')[3])\n",
    "        threat_x = int(threat_at._str.split(':')[0].split('-')[2])\n",
    "        threat_y = int(threat_at._str.split(':')[0].split('-')[3])\n",
    "        target_x = int(target_at._str.split(':')[0].split('-')[2])\n",
    "        target_y = int(target_at._str.split(':')[0].split('-')[3])        \n",
    "        print('Drone-At[{0},{1}], Drone-To[{2}], Threat-At[{5},{6}], Target-At[{7},{8}], Goal-At[{3},{4}]'.format(drone_x, drone_y, drone_to, goal_x, goal_y, threat_x, threat_y, target_x, target_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def Q_learning(self, alpha = 0.01, num_episodes=10001, epsilon=0.2, gamma = 0.90, decay = 0.00002):\n",
    "        games_reward = []\n",
    "        test_rewards = []\n",
    "        episodes = []\n",
    "        initial_epsilon = epsilon\n",
    "        test_env = pddlgym.make(\"PDDLEnvDroneTest-v0\", seed = int(time.time()))\n",
    "\n",
    "        test_reward = self.run_episodes(test_env, 30)\n",
    "        test_rewards.append(test_reward)\n",
    "        episodes.append(0)\n",
    "        total_steps = 0\n",
    "        for ep in range(1, num_episodes):\n",
    "            obs , _ = self.env.reset()\n",
    "            tabular_state = self._convert_obs(obs)\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            self.target_acquired = False\n",
    "            step_count = 0\n",
    "\n",
    "            if epsilon > 0.001 and ep > 4000 :\n",
    "                epsilon -= decay\n",
    "            \n",
    "            while not done:\n",
    "                step_count += 1\n",
    "                valid_actions = list(sorted(self.env.action_space.all_ground_literals(obs, valid_only=True)))\n",
    "                if (len(valid_actions)== 0):\n",
    "                    print(\"Running pisode is stuck, a new episode start\")\n",
    "                    done == True\n",
    "                    break\n",
    "                action, index = self.e_greedy(tabular_state, epsilon, valid_actions)\n",
    "                \n",
    "                if action is None:\n",
    "                    print(\"Running pisode is stuck, a new episode start\")\n",
    "                    done == True\n",
    "                    break\n",
    "                \n",
    "                next_obs, reward, done, _ = self.env.step(action)\n",
    "                          \n",
    "                next_tabular_state  = self._convert_obs(next_obs)\n",
    "\n",
    "                # Target 획득이 필요한 Problem에 대한 Reward\n",
    "                if (self.num_target > 0):\n",
    "                    if ((not done) and (not self.target_acquired)):\n",
    "                        reward = 10\n",
    "                    elif (done and self.target_acquired):\n",
    "                        reward = 20\n",
    "\n",
    "                next_max_q = np.max(np.array(self.action_value[next_tabular_state]))\n",
    "                if (next_max_q != np.NINF) and (self.action_value[tabular_state][index] != np.NINF):\n",
    "                    try:\n",
    "                        self.action_value[tabular_state][index] = self.action_value[tabular_state][index]\\\n",
    "                                                           + alpha * (reward + gamma * next_max_q\\\n",
    "                                                                      - self.action_value[tabular_state][index])\n",
    "                    except RuntimeWarning as e:\n",
    "                        print(\"Warning Raised : \", str(e))\n",
    "                else:\n",
    "                    a = self.action_value[tabular_state][index]\n",
    "                    print(a)\n",
    "\n",
    "                obs = next_obs\n",
    "                tabular_state = next_tabular_state\n",
    "                total_reward += reward\n",
    "\n",
    "                if (done):\n",
    "                    games_reward.append(total_reward)\n",
    "            total_steps += step_count\n",
    "            if (ep % 100) == 0:\n",
    "                test_reward = self.run_episodes(test_env, 30)\n",
    "                print(\"Learning Test Episode:{:5d}  Eps:{:2.4f}  Rew:{:2.4f}\".format(ep, epsilon, test_reward))\n",
    "                print(\"Total Visited States : {}\", len(self.action_value))\n",
    "                print(\"Total Steps : {}\", total_steps)\n",
    "                test_rewards.append(test_reward)\n",
    "                episodes.append(ep)\n",
    "\n",
    "        plt.plot(episodes, test_rewards, 'b-')  # 파란색 실선으로 그래프 그리기\n",
    "        plt.ylim([-50,0])\n",
    "        plt.xlabel('Episode')  # x축 레이블 설정\n",
    "        plt.ylabel('Average Reward')  # y축 레이블 설정\n",
    "        plt.title('Average Test Reward per 100 Episodes\\nalpha={0}, initial_epsilon={1}, gamma = {2}, decay={3}'.format(alpha, initial_epsilon, gamma, decay))  # 그래프 제목 설정\n",
    "        plt.show()  # 그래프 출력1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def Test(self):\n",
    "        env = pddlgym.make(\"PDDLEnvDroneTest-v0\", seed = int(time.time()))\n",
    "        test_reward = self.run_episodes(self.env, 30, True)\n",
    "        print(\"Total Reward of 10 tests:{0}\".format(test_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     env \u001b[39m=\u001b[39m pddlgym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mPDDLEnvDrone-v0\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     rl \u001b[39m=\u001b[39m TabularRL(env, default_action_value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m30.\u001b[39m, num_target\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     rl\u001b[39m.\u001b[39mQ_learning()\n",
      "File \u001b[0;32m~/project/pddlgymRL/pddlgym/__init__.py:19\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     18\u001b[0m     \u001b[39m# env checker fails since obs is not an numpy array like object\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m*\u001b[39;49margs, disable_env_checker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/project/pddlgymRL/.venv/lib/python3.10/site-packages/gym/envs/registration.py:652\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    646\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\n\u001b[1;32m    647\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou passed render_mode=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m'\u001b[39m\u001b[39m although \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt implement human-rendering natively. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    648\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mGym tried to apply the HumanRendering wrapper but it looks like your environment is using the old \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    649\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrendering API, which is not supported by the HumanRendering wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    650\u001b[0m         )\n\u001b[1;32m    651\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 652\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    654\u001b[0m \u001b[39m# Copies the environment creation specification and kwargs to add to the environment specification details\u001b[39;00m\n\u001b[1;32m    655\u001b[0m spec_ \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(spec_)\n",
      "File \u001b[0;32m~/project/pddlgymRL/.venv/lib/python3.10/site-packages/gym/envs/registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     render_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    639\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 640\u001b[0m     env \u001b[39m=\u001b[39m env_creator(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_kwargs)\n\u001b[1;32m    641\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    642\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    643\u001b[0m         \u001b[39mstr\u001b[39m(e)\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39mgot an unexpected keyword argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[39mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    645\u001b[0m     ):\n",
      "File \u001b[0;32m~/project/pddlgymRL/pddlgym/core.py:313\u001b[0m, in \u001b[0;36mPDDLEnv.__init__\u001b[0;34m(self, domain_file, problem_dir, render, seed, raise_error_on_invalid_action, operators_as_actions, dynamic_action_space)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_problem_dir \u001b[39m=\u001b[39m problem_dir\n\u001b[1;32m    312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_render \u001b[39m=\u001b[39m render\n\u001b[0;32m--> 313\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseed(seed)\n\u001b[1;32m    314\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_error_on_invalid_action \u001b[39m=\u001b[39m raise_error_on_invalid_action\n\u001b[1;32m    315\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moperators_as_actions \u001b[39m=\u001b[39m operators_as_actions\n",
      "File \u001b[0;32m~/project/pddlgymRL/pddlgym/core.py:402\u001b[0m, in \u001b[0;36mPDDLEnv.seed\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseed\u001b[39m(\u001b[39mself\u001b[39m, seed):\n\u001b[1;32m    401\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_seed \u001b[39m=\u001b[39m seed\n\u001b[0;32m--> 402\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrng \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mRandomState(seed)\n",
      "File \u001b[0;32mmtrand.pyx:184\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_mt19937.pyx:129\u001b[0m, in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mbit_generator.pyx:519\u001b[0m, in \u001b[0;36mnumpy.random.bit_generator.BitGenerator.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mbit_generator.pyx:309\u001b[0m, in \u001b[0;36mnumpy.random.bit_generator.SeedSequence.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mbit_generator.pyx:383\u001b[0m, in \u001b[0;36mnumpy.random.bit_generator.SeedSequence.get_assembled_entropy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mbit_generator.pyx:138\u001b[0m, in \u001b[0;36mnumpy.random.bit_generator._coerce_to_uint32_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mbit_generator.pyx:77\u001b[0m, in \u001b[0;36mnumpy.random.bit_generator._int_to_uint32_array\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = pddlgym.make(\"PDDLEnvDrone-v0\")\n",
    "    rl = TabularRL(env, default_action_value=-30., num_target=1)\n",
    "    rl.Q_learning()\n",
    "    rl.Test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
